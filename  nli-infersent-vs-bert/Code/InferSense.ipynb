{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4daa3b671a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocesing\n",
    "* Load Dataset\n",
    "* Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 24810: expected 15 fields, saw 16\\nSkipping line 33961: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 75911: expected 15 fields, saw 16\\nSkipping line 100114: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 150638: expected 15 fields, saw 16\\nSkipping line 158834: expected 15 fields, saw 16\\nSkipping line 173104: expected 15 fields, saw 16\\nSkipping line 178252: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 221951: expected 15 fields, saw 16\\n'\n",
      "b'Skipping line 286845: expected 15 fields, saw 16\\nSkipping line 314110: expected 15 fields, saw 16\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391120</th>\n",
       "      <td>2</td>\n",
       "      <td>Clearly, California can - and must - do better.</td>\n",
       "      <td>California cannot do any better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391121</th>\n",
       "      <td>0</td>\n",
       "      <td>It was once regarded as the most beautiful str...</td>\n",
       "      <td>So many of the original buildings had been rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391122</th>\n",
       "      <td>1</td>\n",
       "      <td>Houseboats are a beautifully preserved traditi...</td>\n",
       "      <td>The tradition of houseboats originated while t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391123</th>\n",
       "      <td>0</td>\n",
       "      <td>Obituaries fondly recalled his on-air debates ...</td>\n",
       "      <td>The obituaries were beautiful and written in k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391124</th>\n",
       "      <td>0</td>\n",
       "      <td>in that other you know uh that i should do it ...</td>\n",
       "      <td>My husband has been so overworked lately that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391125 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gold_label                                          sentence1  \\\n",
       "0                0  Conceptually cream skimming has two basic dime...   \n",
       "1                1  you know during the season and i guess at at y...   \n",
       "2                1  One of our number will carry out your instruct...   \n",
       "3                1  How do you know? All this is their information...   \n",
       "4                0  yeah i tell you what though if you go price so...   \n",
       "...            ...                                                ...   \n",
       "391120           2    Clearly, California can - and must - do better.   \n",
       "391121           0  It was once regarded as the most beautiful str...   \n",
       "391122           1  Houseboats are a beautifully preserved traditi...   \n",
       "391123           0  Obituaries fondly recalled his on-air debates ...   \n",
       "391124           0  in that other you know uh that i should do it ...   \n",
       "\n",
       "                                                sentence2  \n",
       "0       Product and geography are what make cream skim...  \n",
       "1       You lose the things to the following level if ...  \n",
       "2       A member of my team will execute your orders w...  \n",
       "3                       This information belongs to them.  \n",
       "4                The tennis shoes have a range of prices.  \n",
       "...                                                   ...  \n",
       "391120                   California cannot do any better.  \n",
       "391121  So many of the original buildings had been rep...  \n",
       "391122  The tradition of houseboats originated while t...  \n",
       "391123  The obituaries were beautiful and written in k...  \n",
       "391124  My husband has been so overworked lately that ...  \n",
       "\n",
       "[391125 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#URL='/content/drive/MyDrive/multinli_1.0/'\n",
    "URL='multinli_1.0/'\n",
    "train = pd.read_csv(URL+'multinli_1.0_train.txt', delimiter = \"\\t\",error_bad_lines=False)\n",
    "dev_matched = pd.read_csv(URL+'multinli_1.0_dev_matched.txt', delimiter = \"\\t\",error_bad_lines=False)\n",
    "dev_mismatched = pd.read_csv(URL+'multinli_1.0_dev_mismatched.txt', delimiter = \"\\t\",error_bad_lines=False)\n",
    "\n",
    "train = train[['gold_label', 'sentence1', 'sentence2']]\n",
    "dev_matched = dev_matched[['gold_label', 'sentence1', 'sentence2']]\n",
    "dev_mismatched = dev_mismatched[['gold_label', 'sentence1', 'sentence2']]\n",
    "\n",
    "# Drop the missing row\n",
    "train = train.dropna()\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# Convert categorical label\n",
    "train['gold_label'] = pd.factorize(train['gold_label'])[0]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment\n",
    "## 2.1. import InferSense\n",
    "* Import pretrained model\n",
    "    + version 1 (GloVe) or\n",
    "    + version 2 (fastText)\n",
    "* create iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InferSense_models import InferSent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = 1 # 1 is trained with GloVe, 2 with fastText\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model_emb = InferSent(params_model)\n",
    "model_emb.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model_emb = model_emb.cuda() if use_cuda else model_emb\n",
    "\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = 'GloVe/glove.840B.300d.txt' if model_version == 1 else 'fastText/crawl-300d-2M.vec'\n",
    "model_emb.set_w2v_path(W2V_PATH)\n",
    "\n",
    "# Load embeddings of K most frequent words\n",
    "model_emb.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04041788, 0.13023938, 0.05888408, ..., 0.00592342, 0.        ,\n",
       "       0.01562545], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 19569/22158 (88.3%)\n",
      "Speed : 57.4 sentences/s (cpu mode, bsize=128)\n",
      "nb sentences encoded : 999\n"
     ]
    }
   ],
   "source": [
    "embeddings = model_emb.encode(train['sentence1'][1:1000].to_list(), bsize=128, tokenize=False, verbose=True)\n",
    "print('nb sentences encoded : {0}'.format(len(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator_func(X1,X2,y,batch_size):\n",
    "    size = len (X)\n",
    "    permutation = np.random.permutation(size)#randomize the values so the same class is not in only one batch\n",
    "    iterator = []\n",
    "    for i in range(0,size,batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = {}\n",
    "        batch['sentence1'] = [X1[i] for i in indices]\n",
    "        batch['sentence2'] = [X2[i] for i in indices]\n",
    "        batch['label'] = [y[i] for i in indices]\n",
    "        \n",
    "        #order by length\n",
    "        batch['sentence1'],batch['sentence2'],batch['label'] = zip(*sorted(zip(\n",
    "            batch['sentence1'],batch['sentence2'],batch['label']),key = lambda x: len(x[0])+len(x[1]), reverse = True))\n",
    "        batch ['lenght'] = [len(batch['sentence1'][j])+len(batch['sentence1'][j]) for j in range(batch_size)]\n",
    "        \n",
    "        # to tensor\n",
    "        batch['lenght'] = torch.IntTensor(batch['lenght'])\n",
    "        batch['sentence1'] = torch.Tensor(batch['sentence1'])\n",
    "        batch['sentence2'] = torch.Tensor(batch['sentence2'])\n",
    "        batch['label'] = torch.Tensor(batch['label'])\n",
    "        #batch['sentence1'] = torch.nn.utils.rnn.pad_sequence(batch['sentence1'],batch_first=True).t()\n",
    "        \n",
    "        #to device\n",
    "        batch['lenght'] = batch['lenght'].to(device)\n",
    "        batch['sentence1'] = batch['sentence1'].to(device)\n",
    "        batch['sentence2'] = batch['sentence2'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "        \n",
    "        iterator.append(batch)\n",
    "    \n",
    "    return iterator \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator_batch_func(X1,X2,y, model_emb, batch_size):\n",
    "    \"\"\"\n",
    "    it creates a batch preprocecced with teh given embedding model. The inputs are:\n",
    "    - X1: sentences 1 filtered by the batch selected\n",
    "    - X2: sentences 1 filtered by the batch selected\n",
    "    - y: labels filtered by the batch selected\n",
    "    - emb_model: model used to trandomr sentence 1 and 2 to the embedding space    \n",
    "    \"\"\"\n",
    "    \n",
    "    batch = {}\n",
    "    # apply embedding        \n",
    "    batch['sentence1'] =  model_emb.encode(X1.to_list(), bsize=batch_size, tokenize=False, verbose=False)\n",
    "    batch['sentence2'] =  model_emb.encode(X1.to_list(), bsize=batch_size, tokenize=False, verbose=False)\n",
    "    batch['label'] = y\n",
    "    \n",
    "    batch['sentence1'] = torch.Tensor(batch['sentence1'])\n",
    "    batch['sentence2'] = torch.Tensor(batch['sentence2'])\n",
    "    batch['label'] = torch.Tensor(batch['label'])\n",
    "    #batch['sentence1'] = torch.nn.utils.rnn.pad_sequence(batch['sentence1'],batch_first=True).t()\n",
    "\n",
    "    batch['lenght'] = batch['lenght'].to(device)\n",
    "    batch['sentence1'] = batch['sentence1'].to(device)\n",
    "    batch['sentence2'] = batch['sentence2'].to(device)\n",
    "    batch['label'] = batch['label'].to(device)\n",
    "    \n",
    "    return batch \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Model Definition\n",
    "* define model\n",
    "* create instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0393216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048*64*300/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defin the NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instace\n",
    "input_DIM = ..\n",
    "embedding_dim = ..\n",
    "Hidden_dim = ..\n",
    "Output_dim = ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Train the model\n",
    "* training Parameters\n",
    "    + optimizer: SGD\n",
    "    + criterion: Cross Entropy Loss\n",
    "    + Device: cuda or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128# size of the cuda...\n",
    "device = torch.device ('cda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3055.6640625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)/128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
