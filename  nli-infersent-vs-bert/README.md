# ğŸ§  Natural Language Inference: InferSent vs BERT (UCL MSc Project)

As part of my MSc in Data Science at UCL, I led a research project analyzing the performance and architecture of models designed for **Natural Language Inference (NLI)**, using the **MultiNLI corpus**.

We evaluated various encoder types (LSTM, BiLSTM, BiGRU, Self-Attention, and BERT) within a consistent architecture, focusing on:

- Sentence Embedding Methods: Classical word em via concatenation, subtraction, multiplication
- Classifier Structures: Linear and nonlinear laybeddings (GloVe, FastText) vs Transformer-based representations (BERT)
- Vector Merging Strategies: Combining embeddingsers, dropout, and layer depth

ğŸ§  Key Outcomes:

- BERT outperformed other models by a large margin (+15% accuracy)
- Vector merging (e.g., using u*v, |uâˆ’v|) significantly boosted performance
- The classifier layer structure had minimal impact, emphasizing the importance of good encoders and merging blocks
- Statistical tests validated model differences (e.g., BiLSTM > BiGRU)

---

## ğŸ§© What is Natural Language Inference (NLI)?

NLI is the task of determining the **logical relationship** between a pair of sentences:

- **Premise**: The original sentence
- **Hypothesis**: A follow-up sentence that may be related to the premise
- **Label**: One of:
  - **Entailment** â€“ Hypothesis must be true
  - **Contradiction** â€“ Hypothesis cannot be true
  - **Neutral** â€“ Hypothesis may or may not be true

Example:
> **Premise:** â€œThe man is playing a guitar.â€  
> **Hypothesis:** â€œA man is performing music.â€  
> **Label:** Entailment

This task is foundational for sentence-level understanding and powers applications like:
- Fact-checking
- Conversational AI
- Semantic search

### ğŸ–¼ï¸ NLI Training Workflow

![NLI Training Scheme](generic-nli-training-scheme.png)

---

## ğŸ“Š Key Results

| Encoder        | Matched | Mismatched |
|----------------|---------|------------|
| LSTM           | 63.25%  | 64.06%     |
| BiGRU          | 68.55%  | 67.99%     |
| BiLSTM         | 69.11%  | 69.02%     |
| Self-Attention | 69.44%  | 69.01%     |
| BERT (base)    | **83.29%** | **83.30%** |

- BERT outperforms all traditional encoders
- Merge vector strategy (|uâˆ’v|, u*v) strongly impacts performance
- Classifier design has **minimal impact** beyond a threshold

---

## ğŸ§  Core Learnings

- Quality of sentence encoders dominates downstream performance
- Vector merging is not just implementation detailâ€”it drives the signal
- Even strong statistical models (like BiLSTM + FastText) fall behind pretrained Transformers
- Testing across *matched* and *mismatched* domains reveals robustness

---

## ğŸ“‚ Repo Structure

```
nli-infersent-vs-bert/
â”œâ”€â”€ Code/                  # Model implementations and training scripts
â”œâ”€â”€ InferSenseNLI/         # Training logs, configs, checkpoints
â”œâ”€â”€ report/
â”‚   â””â”€â”€ UCL_NLI_Report.pdf  # Full academic write-up
â””â”€â”€ README.md              # This file
```

---

## ğŸ§ª How to Run

### 1. Clone the repo
```bash
git clone https://github.com/agonzalezp2/nli-infersent-vs-bert
cd nli-infersent-vs-bert
```

### 2. Install dependencies (via `requirements.txt` or environment.yaml if provided)
```bash
pip install -r requirements.txt
```

### 3. Train & Evaluate
```bash
python train.py --model BiLSTM --embed fasttext
```

Or run evaluation on BERT baseline:
```bash
python train.py --model BERT --dataset MultiNLI
```

---

## ğŸ“˜ References
- Williams, Adina et al. (2018). "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference."
- Conneau et al. (2017). "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data."

---

## ğŸ‘¤ About Me
I'm AgustÃ­n, a Data & AI professional based in London. I build data products that balance academic rigor with business impact.

- [Portfolio](https://github.com/agonzalezp2/agustin-portfolio)
- [LinkedIn](https://www.linkedin.com/in/agustin-gonzalez-pozo)
